{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the two csv files\n",
    "tweet_df = pd.read_csv(\"../data/tweet_data.csv\")\n",
    "stock_df = pd.read_csv(\"../data/stock_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the date column in the stock data to datetime and keep the date \n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "stock_df['Date'] = stock_df['Date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the date column in the tweet data to datetime and keep the date\n",
    "tweet_df['Date'] = pd.to_datetime(tweet_df['Date'])\n",
    "tweet_df['Date'] = tweet_df['Date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the full company name column from the tweet data\n",
    "tweet_df.drop('Company Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Data Columns: \n",
      "Date\n",
      "Tweet\n",
      "Stock Name\n",
      "\n",
      "Stock Data Columns: \n",
      "Date\n",
      "Open\n",
      "High\n",
      "Low\n",
      "Close\n",
      "Adj Close\n",
      "Volume\n",
      "Stock Name\n"
     ]
    }
   ],
   "source": [
    "# print the columns of the two dataframes\n",
    "print(\"Tweet Data Columns: \")\n",
    "for col in tweet_df.columns:\n",
    "    print(col)\n",
    "print()\n",
    "print(\"Stock Data Columns: \")\n",
    "for col in stock_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates in both data:  252\n"
     ]
    }
   ],
   "source": [
    "# check how many dates figure in both dataframes\n",
    "print(\"Number of dates in both data: \", len(set(tweet_df['Date'].unique()) & set(stock_df['Date'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two dataframes on the date column\n",
    "joined_df = pd.merge(tweet_df, stock_df, \n",
    "                     on=['Date', 'Stock Name'], \n",
    "                     how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Data Columns: \n",
      "Date\n",
      "Tweet\n",
      "Stock Name\n",
      "Open\n",
      "High\n",
      "Low\n",
      "Close\n",
      "Adj Close\n",
      "Volume\n",
      "Joined Data Shape:  (80793, 9)\n"
     ]
    }
   ],
   "source": [
    "# print the columns of the joined dataframe\n",
    "print(\"Joined Data Columns: \")\n",
    "for col in joined_df.columns:\n",
    "    print(col)\n",
    "\n",
    "# print the shape of the joined dataframe\n",
    "print(\"Joined Data Shape: \", joined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dataframe to a csv file\n",
    "joined_df.to_csv(\"../data/joined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the new dataframe from the csv file\n",
    "df = pd.read_csv(\"../data/joined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: \n",
      "Date 0\n",
      "Tweet 0\n",
      "Stock Name 0\n",
      "Open 17117\n",
      "High 17117\n",
      "Low 17117\n",
      "Close 17117\n",
      "Adj Close 17117\n",
      "Volume 17117\n"
     ]
    }
   ],
   "source": [
    "# get a rundown of the missing values per column\n",
    "print(\"Missing Values: \")\n",
    "for col in df.columns:\n",
    "    print(col, df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of the dataframe:  (63676, 9)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the dataframe\n",
    "print(\"New shape of the dataframe: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elaty\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# get a list of the stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(hashtag):\n",
    "    # Splitting based on the presence of capital letters (camel case)\n",
    "    parts = re.findall('[A-Z][^A-Z]*', hashtag)\n",
    "    # If the hashtag is all lowercase, just return it as is.\n",
    "    if not parts:\n",
    "        return hashtag\n",
    "    # Re-join the parts with a space\n",
    "    return ' '.join(part for part in parts)\n",
    "\n",
    "# Process hashtags\n",
    "def replace_with_processed(match):\n",
    "    hashtag = match.group(1)\n",
    "    return split_hashtag(hashtag)\n",
    "\n",
    "# create a function to clean the text of a tweet\n",
    "def clean_tweet(tweet):\n",
    "    # Convert emojis to words\n",
    "    tweet = emoji.demojize(tweet)\n",
    "\n",
    "    # Replace hashtags in the tweet with their processed forms\n",
    "    tweet = re.sub(r'#(\\w+)', replace_with_processed, tweet)\n",
    "    \n",
    "    # Lowercase the tweet\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Remove stopwords (optional, based on your preference)\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user @ references\n",
    "    tweet = re.sub(r'\\@\\w+','', tweet)\n",
    "    \n",
    "    # Remove punctuations and special characters (except $ and %)\n",
    "    tweet = re.sub(r'[^\\w\\s\\$%]', '', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the clean_tweet function on the tweet text column\n",
    "df['Tweet'] = df['Tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tweet Example:  mainstream media done amazing job brainwashing people today work asked companies believe amp said  make safest cars amp everyone disagreed heardthey catch fire amp batteries cost 20k replace\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned Tweet Example: \", df['Tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned dataframe to a csv file\n",
    "df.to_csv(\"../data/cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the cleaned data from the csv file\n",
    "df = pd.read_csv(\"../data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our scaler objects\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the columns in [Open, High, Low, Close, Adj Close, Volume], apply the scalers\n",
    "for col in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']:\n",
    "    df[col] = minmax_scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    df[col] = standard_scaler.fit_transform(df[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elaty\\AppData\\Local\\Programs\\Python\\Venvs\\Machine Learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# for the Volume column, apply the log transformation\n",
    "df['Volume'] = np.log(df['Volume'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date                                              Tweet Stock Name  \\\n",
      "0  2022-09-29  mainstream media done amazing job brainwashing...       TSLA   \n",
      "1  2022-09-29  tesla delivery estimates around 364k analysts ...       TSLA   \n",
      "2  2022-09-29  3 even include 630m unvested rsus 630 addition...       TSLA   \n",
      "3  2022-09-29     hahaha still trying stop tesla fsd bro get ...       TSLA   \n",
      "4  2022-09-29         stop trying kill kids sad deranged old man       TSLA   \n",
      "\n",
      "       Open      High       Low    Close  Adj Close   Volume  \n",
      "0  0.506929  0.454661  0.411441  0.37235    0.37616  0.25447  \n",
      "1  0.506929  0.454661  0.411441  0.37235    0.37616  0.25447  \n",
      "2  0.506929  0.454661  0.411441  0.37235    0.37616  0.25447  \n",
      "3  0.506929  0.454661  0.411441  0.37235    0.37616  0.25447  \n",
      "4  0.506929  0.454661  0.411441  0.37235    0.37616  0.25447  \n"
     ]
    }
   ],
   "source": [
    "# print the first 5 rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/standard_scaler.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the scaler objects to disk\n",
    "joblib.dump(minmax_scaler, '../models/minmax_scaler.pkl')\n",
    "joblib.dump(standard_scaler, '../models/standard_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv(\"../data/scaled_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
